# -*- coding: utf-8 -*-
"""SMS Spam Detection AI project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19DMsdxyFHd5Rr5gevKtSyEEdgw86nLkI

sms_spam_detection.py
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import string
import nltk
from nltk.corpus import stopwords
import re

nltk.download('stopwords')
stop_words = stopwords.words('english')

"""Loading Dataset"""

data = pd.read_csv("https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv", sep='\t')
data.columns = ['label', 'message']

"""Preprocessing Text"""

def clean_text(text):
    text = text.lower()
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)
    words = [word for word in text.split() if word not in stop_words]
    return " ".join(words)

data['clean_message'] = data['message'].apply(clean_text)

"""Converting Text to Numbers"""

cv = CountVectorizer()
X = cv.fit_transform(data['clean_message'])
y = data['label']

"""Train/Test Spliting"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""Training Model"""

model = MultinomialNB()
model.fit(X_train, y_train)

"""Evaluateing Model"""

y_pred = model.predict(X_test)
print("âœ… Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

"""Testing with Custom Message"""

msg = ["Congratulations! You won a free iPhone!"]
msg_vec = cv.transform(msg)
print("\nMessage:", msg[0])
print("Prediction:", model.predict(msg_vec)[0])

pip install pandas scikit-learn nltk streamlit

"""App.py"""

import streamlit as st
import pickle
import string
import re
from nltk.corpus import stopwords
import nltk

nltk.download('stopwords')
stop_words = stopwords.words('english')

"""Loading model and vectorizer"""

model = pickle.load(open("spam_model.pkl", "rb"))
vectorizer = pickle.load(open("vectorizer.pkl", "rb"))

"""Cleaning function"""

def clean_text(text):
    text = text.lower()
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)
    words = [word for word in text.split() if word not in stop_words]
    return " ".join(words)

"""Streamlit UI"""

st.title("ðŸ“± SMS Spam Detection App")
st.write("Detect whether a message is **Spam** or **Not Spam** using Machine Learning!")

user_input = st.text_area("Enter your message:")

if st.button("Check Message"):
    cleaned = clean_text(user_input)
    vectorized = vectorizer.transform([cleaned])
    prediction = model.predict(vectorized)[0]

    if prediction == "spam":
        st.error("ðŸš¨ This message looks like **Spam!**")
    else:
        st.success("âœ… This message looks **Safe (Not Spam)**!")